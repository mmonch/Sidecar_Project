{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmonch/Sidecar_Project/blob/main/notebooks/Sidecar_Project_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "TcCMqbiVYnFl"
      },
      "source": [
        "<a id='Q0'></a>\n",
        "<center><a target=\"_blank\" href=\"https://sit.academy/\"><img src=\"https://drive.google.com/uc?id=1z0U84GYqhbWWpCenFajh8_8XFRGyOc3U\" width=\"200\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
        "<center> <h1> Notebook 2: Seq2Seq Model </h1> </center>\n",
        "<p style=\"margin-bottom:1cm;\"></p>\n",
        "<center><h4>Marlies Monch, SIT Academy, 2022</h4></center>\n",
        "<p style=\"margin-bottom:1cm;\"></p>\n",
        "\n",
        "<div style=\"background:#EEEDF5;border-top:0.1cm solid #EF475B;border-bottom:0.1cm solid #EF475B;\">\n",
        "    <div style=\"margin-left: 0.5cm;margin-top: 0.5cm;margin-bottom: 0.5cm;color:#303030\">\n",
        "        <p><strong>Goal:</strong> Run a seq2seq model (RNN) on the attribute technical names to match the attribute business names</p>\n",
        "        <strong> Outline:</strong>\n",
        "        <a id='P0' name=\"P0\"></a>\n",
        "        <ol>\n",
        "            <li> <a style=\"color:#303030\" href='#I'>Introduction </a> </li>\n",
        "            <li> <a style=\"color:#303030\" href='#SU'>Set up</a></li>\n",
        "            <li> <a style=\"color:#303030\" href='#P1'>Part 1</a></li>\n",
        "            <li> <a style=\"color:#303030\" href='#P2'>Part 2</a></li>\n",
        "            <li> <a style=\"color:#303030\" href='#P3'>Part 3</a></li>\n",
        "            <li> <a style=\"color:#303030\" href='#CL'>Conclusion</a></li>\n",
        "        </ol>\n",
        "        <strong>Keywords:</strong> data preprocessing, seq2seq, NLP, Sidecar attribute names.\n",
        "    </div>\n",
        "</div>\n",
        "</nav>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86cmQzAYnFn"
      },
      "source": [
        "<a id='I' name=\"I\"></a>\n",
        "## [Introduction](#P0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7o-oAEuYnFn"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n",
        "\n",
        "https://loeb.nyc/blog/data-science-word-expander\n",
        "\n",
        "https://towardsdatascience.com/nlp-building-text-cleanup-and-preprocessing-pipeline-eba4095245a0\n",
        "\n",
        "https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e\n",
        "\n",
        "https://www.machinecurve.com/index.php/2020/12/29/differences-between-autoregressive-autoencoding-and-sequence-to-sequence-models-in-machine-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XENd8ZgPYnFo"
      },
      "source": [
        "<a id='SU' name=\"SU\"></a>\n",
        "## [Set up](#P0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUSprQRpYnFo"
      },
      "source": [
        "### Magics and Package Installations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nb_black\n",
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "!pip install --upgrade IPython"
      ],
      "metadata": {
        "id": "nuw3F8xNcWNx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae714279-264b-4434-b02a-75a69e2f0201"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nb_black\n",
            "  Downloading nb_black-1.0.7.tar.gz (4.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from nb_black) (5.5.0)\n",
            "Collecting black>='19.3'\n",
            "  Downloading black-21.12b0-py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting tomli<2.0.0,>=0.2.6\n",
            "  Downloading tomli-1.2.3-py3-none-any.whl (12 kB)\n",
            "Collecting pathspec<1,>=0.9.0\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black>='19.3'->nb_black) (3.10.0.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black>='19.3'->nb_black) (7.1.2)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 63.3 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting platformdirs>=2\n",
            "  Downloading platformdirs-2.4.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->nb_black) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->nb_black) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->nb_black) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->nb_black) (0.7.0)\n",
            "Building wheels for collected packages: nb-black\n",
            "  Building wheel for nb-black (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nb-black: filename=nb_black-1.0.7-py3-none-any.whl size=5297 sha256=b2b8245e358d0def1b4deeffa4efe33aae99e2be9be9a342ad9db77a3c08b656\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/b2/88/51c66d23ea5fd0d40ed50997555e15d981d92671376a9a412a\n",
            "Successfully built nb-black\n",
            "Installing collected packages: typed-ast, tomli, platformdirs, pathspec, mypy-extensions, black, nb-black\n",
            "  Attempting uninstall: tomli\n",
            "    Found existing installation: tomli 2.0.0\n",
            "    Uninstalling tomli-2.0.0:\n",
            "      Successfully uninstalled tomli-2.0.0\n",
            "Successfully installed black-21.12b0 mypy-extensions-0.4.3 nb-black-1.0.7 pathspec-0.9.0 platformdirs-2.4.1 tomli-1.2.3 typed-ast-1.5.2\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 60.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85461 sha256=f6fca3d6133f6590cc18ac924683037d42272ac578add7863f9bba339df95ae3\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Collecting IPython\n",
            "  Downloading ipython-7.31.1-py3-none-any.whl (792 kB)\n",
            "\u001b[K     |████████████████████████████████| 792 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.24-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from IPython) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from IPython) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython) (2.6.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from IPython) (0.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->IPython) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.5)\n",
            "Installing collected packages: prompt-toolkit, IPython\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: IPython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.24 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.31.1 which is incompatible.\u001b[0m\n",
            "Successfully installed IPython-7.31.1 prompt-toolkit-3.0.24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "axuCywGJYnFo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0bb0ddb0-1e60-4420-a8b8-a37368407280"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 2;\n",
              "                var nbb_unformatted_code = \"# auto reload packages and modules when they are modified\\n%load_ext autoreload\\n%autoreload 2\\n# draw matplotlib plots in line\\n%matplotlib inline\\n# enforce PEP 8 code on jupyter lab ...\\n#%load_ext lab_black\\n# ... or jupyter notebook\\n%load_ext nb_black\";\n",
              "                var nbb_formatted_code = \"# auto reload packages and modules when they are modified\\n%load_ext autoreload\\n%autoreload 2\\n# draw matplotlib plots in line\\n%matplotlib inline\\n# enforce PEP 8 code on jupyter lab ...\\n#%load_ext lab_black\\n# ... or jupyter notebook\\n%load_ext nb_black\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# auto reload packages and modules when they are modified\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# draw matplotlib plots in line\n",
        "%matplotlib inline\n",
        "# enforce PEP 8 code on jupyter lab ...\n",
        "#%load_ext lab_black\n",
        "# ... or jupyter notebook\n",
        "%load_ext nb_black"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwYad_o4YnFp"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-07T16:56:09.469904Z",
          "start_time": "2019-01-07T16:56:07.858398Z"
        },
        "id": "xX5dlhbTYnFp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "afaede41-f5e4-4413-d562-fe2a8d91d6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 36;\n",
              "                var nbb_unformatted_code = \"import nltk\\nimport pandas as pd\\nimport numpy as np\\nimport tqdm\\nimport tensorflow as tf\\nimport unicodedata\\nimport re\\nimport contractions\\nimport sklearn\\nfrom tensorflow.keras import layers\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras.layers import Flatten\\nfrom tensorflow.keras.layers import Conv1D\\nfrom tensorflow.keras.layers import MaxPooling1D\\nfrom tensorflow.keras.layers import Embedding\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing import sequence\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.layers import TextVectorization\\n\\n# from nltk get \\\"punkt\\\"\\nnltk.download('punkt')\\n\\n# fix random seed for reproducibility\\nseed = 42\\nnp.random.seed(seed)\";\n",
              "                var nbb_formatted_code = \"import nltk\\nimport pandas as pd\\nimport numpy as np\\nimport tqdm\\nimport tensorflow as tf\\nimport unicodedata\\nimport re\\nimport contractions\\nimport sklearn\\nfrom tensorflow.keras import layers\\nfrom tensorflow import keras\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras.layers import Flatten\\nfrom tensorflow.keras.layers import Conv1D\\nfrom tensorflow.keras.layers import MaxPooling1D\\nfrom tensorflow.keras.layers import Embedding\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\nfrom tensorflow.keras.preprocessing import sequence\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.layers import TextVectorization\\n\\n# from nltk get \\\"punkt\\\"\\nnltk.download(\\\"punkt\\\")\\n\\n# fix random seed for reproducibility\\nseed = 42\\nnp.random.seed(seed)\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import re\n",
        "import contractions\n",
        "import sklearn\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# from nltk get \"punkt\"\n",
        "nltk.download('punkt')\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPEMrKo9YnFp"
      },
      "source": [
        "### Custom classes and functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmFlMudQYnFq"
      },
      "source": [
        "### Global Parameters Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyIHEoLBYnFr"
      },
      "source": [
        "### User-Dependent Variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "_6RoD2gWdVhW",
        "outputId": "ed8cfe80-d331-4396-d6d0-ce34d729c789"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 4;\n",
              "                var nbb_unformatted_code = \"from google.colab import drive\\ndrive.mount('/content/gdrive')\";\n",
              "                var nbb_formatted_code = \"from google.colab import drive\\n\\ndrive.mount(\\\"/content/gdrive\\\")\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"gdrive/My Drive/SIDECAR_P/Sidecar_Data_Sample.csv\")"
      ],
      "metadata": {
        "id": "nkSRl9EVd5E2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cba6b558-fba6-45c8-e438-2a1fc5ad48c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 5;\n",
              "                var nbb_unformatted_code = \"data = pd.read_csv(\\\"gdrive/My Drive/SIDECAR_P/Sidecar_Data_Sample.csv\\\")\";\n",
              "                var nbb_formatted_code = \"data = pd.read_csv(\\\"gdrive/My Drive/SIDECAR_P/Sidecar_Data_Sample.csv\\\")\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lnhzDc9YnFr"
      },
      "source": [
        "<a id='P1'></a>\n",
        "## [Part one](#P0)\n",
        "\n",
        "Here you could write a paragraph on the aim of part one. You could also have some nice figures\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "-5G9FMbwemgN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "62f8b601-f22a-4339-a1d8-cfadc7f36325"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-966f0b50-5e0f-47b0-af3c-cd8ec14449c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Domain_Id</th>\n",
              "      <th>Domain_Name</th>\n",
              "      <th>Domain_Desc</th>\n",
              "      <th>Domain_Code</th>\n",
              "      <th>Domain_Status</th>\n",
              "      <th>Domain_Status_Code</th>\n",
              "      <th>Application_Id</th>\n",
              "      <th>Application_Name</th>\n",
              "      <th>Application_Code</th>\n",
              "      <th>Application_Desc</th>\n",
              "      <th>Application_Status</th>\n",
              "      <th>Application_Status_Code</th>\n",
              "      <th>Asset_Id</th>\n",
              "      <th>Asset_Technical_Name</th>\n",
              "      <th>Asset_Business_Name</th>\n",
              "      <th>Asset_Business_Desc</th>\n",
              "      <th>Asset_Status</th>\n",
              "      <th>Asset_Status_Code</th>\n",
              "      <th>Asset_Server_Name</th>\n",
              "      <th>Asset_Database</th>\n",
              "      <th>Asset_Schema</th>\n",
              "      <th>Asset_Type_Id</th>\n",
              "      <th>Asset_Type_Name</th>\n",
              "      <th>Asset_Type_Desc</th>\n",
              "      <th>Privacy_Level_Id</th>\n",
              "      <th>Asset_Privacy_Level</th>\n",
              "      <th>Asset_Privacy_Desc</th>\n",
              "      <th>Attribute_Id</th>\n",
              "      <th>Attribute_Technical_Name</th>\n",
              "      <th>Attribute_Business_Name</th>\n",
              "      <th>Attribute_Business_Desc</th>\n",
              "      <th>Attribute_Business_Rule</th>\n",
              "      <th>Attribute_Data_Type</th>\n",
              "      <th>Attribute_IsPK</th>\n",
              "      <th>Attribute_IsNullable</th>\n",
              "      <th>Attribute_Status</th>\n",
              "      <th>Attribute_Status_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Patient administrative</td>\n",
              "      <td>Patient administrative data (demographic &amp; adm...</td>\n",
              "      <td>PAT_ADM</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Patient Manager</td>\n",
              "      <td>PATMAN</td>\n",
              "      <td>Patient Manager tool is the application allowi...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>PMS_PT</td>\n",
              "      <td>Patient</td>\n",
              "      <td>Patient table Contain personal information abo...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>SWT01_PRD</td>\n",
              "      <td>PMS</td>\n",
              "      <td>dbo</td>\n",
              "      <td>1</td>\n",
              "      <td>Table</td>\n",
              "      <td>Database table</td>\n",
              "      <td>4</td>\n",
              "      <td>Level 4 - Highly sensitive</td>\n",
              "      <td>Highly sensitive - Internal use</td>\n",
              "      <td>1</td>\n",
              "      <td>ID</td>\n",
              "      <td>Technical Id of the patient</td>\n",
              "      <td>The Primary Key (surrogate unique identifier) ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NUMBER (38)</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Patient administrative</td>\n",
              "      <td>Patient administrative data (demographic &amp; adm...</td>\n",
              "      <td>PAT_ADM</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Patient Manager</td>\n",
              "      <td>PATMAN</td>\n",
              "      <td>Patient Manager tool is the application allowi...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>PMS_PT</td>\n",
              "      <td>Patient</td>\n",
              "      <td>Patient table Contain personal information abo...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>SWT01_PRD</td>\n",
              "      <td>PMS</td>\n",
              "      <td>dbo</td>\n",
              "      <td>1</td>\n",
              "      <td>Table</td>\n",
              "      <td>Database table</td>\n",
              "      <td>4</td>\n",
              "      <td>Level 4 - Highly sensitive</td>\n",
              "      <td>Highly sensitive - Internal use</td>\n",
              "      <td>2</td>\n",
              "      <td>GNDR_CD</td>\n",
              "      <td>Gender Code</td>\n",
              "      <td>Code of GENDER CODE defined as: The gender of ...</td>\n",
              "      <td>If Gender code = M then Male, F = Female, or U...</td>\n",
              "      <td>VARCHAR2 (80)</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Patient administrative</td>\n",
              "      <td>Patient administrative data (demographic &amp; adm...</td>\n",
              "      <td>PAT_ADM</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Patient Manager</td>\n",
              "      <td>PATMAN</td>\n",
              "      <td>Patient Manager tool is the application allowi...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>PMS_PT</td>\n",
              "      <td>Patient</td>\n",
              "      <td>Patient table Contain personal information abo...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>SWT01_PRD</td>\n",
              "      <td>PMS</td>\n",
              "      <td>dbo</td>\n",
              "      <td>1</td>\n",
              "      <td>Table</td>\n",
              "      <td>Database table</td>\n",
              "      <td>4</td>\n",
              "      <td>Level 4 - Highly sensitive</td>\n",
              "      <td>Highly sensitive - Internal use</td>\n",
              "      <td>3</td>\n",
              "      <td>LIVG_ARNGMNT_CD</td>\n",
              "      <td>Living arrangement</td>\n",
              "      <td>Code of LIVING ARRANGEMENT CODE defined as: A ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>VARCHAR2 (80)</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Patient administrative</td>\n",
              "      <td>Patient administrative data (demographic &amp; adm...</td>\n",
              "      <td>PAT_ADM</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Patient Manager</td>\n",
              "      <td>PATMAN</td>\n",
              "      <td>Patient Manager tool is the application allowi...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>PMS_PT</td>\n",
              "      <td>Patient</td>\n",
              "      <td>Patient table Contain personal information abo...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>SWT01_PRD</td>\n",
              "      <td>PMS</td>\n",
              "      <td>dbo</td>\n",
              "      <td>1</td>\n",
              "      <td>Table</td>\n",
              "      <td>Database table</td>\n",
              "      <td>4</td>\n",
              "      <td>Level 4 - Highly sensitive</td>\n",
              "      <td>Highly sensitive - Internal use</td>\n",
              "      <td>4</td>\n",
              "      <td>MRTL_STUS_CD</td>\n",
              "      <td>Marital status code</td>\n",
              "      <td>Code of MARITAL STATUS CODE defined as: Party'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>VARCHAR2 (80)</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Patient administrative</td>\n",
              "      <td>Patient administrative data (demographic &amp; adm...</td>\n",
              "      <td>PAT_ADM</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Patient Manager</td>\n",
              "      <td>PATMAN</td>\n",
              "      <td>Patient Manager tool is the application allowi...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>PMS_PT</td>\n",
              "      <td>Patient</td>\n",
              "      <td>Patient table Contain personal information abo...</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "      <td>SWT01_PRD</td>\n",
              "      <td>PMS</td>\n",
              "      <td>dbo</td>\n",
              "      <td>1</td>\n",
              "      <td>Table</td>\n",
              "      <td>Database table</td>\n",
              "      <td>4</td>\n",
              "      <td>Level 4 - Highly sensitive</td>\n",
              "      <td>Highly sensitive - Internal use</td>\n",
              "      <td>5</td>\n",
              "      <td>OCUPATN_CD</td>\n",
              "      <td>Occupation code</td>\n",
              "      <td>Code of OCCUPATION CODE defined as: The occupa...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>VARCHAR2 (80)</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Active</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-966f0b50-5e0f-47b0-af3c-cd8ec14449c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-966f0b50-5e0f-47b0-af3c-cd8ec14449c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-966f0b50-5e0f-47b0-af3c-cd8ec14449c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Domain_Id             Domain_Name  ... Attribute_Status Attribute_Status_Code\n",
              "0          1  Patient administrative  ...           Active                     1\n",
              "1          1  Patient administrative  ...           Active                     1\n",
              "2          1  Patient administrative  ...           Active                     1\n",
              "3          1  Patient administrative  ...           Active                     1\n",
              "4          1  Patient administrative  ...           Active                     1\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 6;\n",
              "                var nbb_unformatted_code = \"data.head()\";\n",
              "                var nbb_formatted_code = \"data.head()\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "N5_HnDEFetV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "TramfDjphbLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().any()"
      ],
      "metadata": {
        "id": "ymfwzlv5hkw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_na = data[data.isna().any(axis=1)]"
      ],
      "metadata": {
        "id": "2glQ7Hkzhk8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(is_na)"
      ],
      "metadata": {
        "id": "uk-Wb_mjizsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_na.tail()"
      ],
      "metadata": {
        "id": "zCWh_4CFjPoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sort(data[\"Asset_Id\"].unique())"
      ],
      "metadata": {
        "id": "ZEBEkQ38khuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how many values per column, value_counts() per category\n",
        "for col in data.columns:\n",
        "  print (f\"Column Name, Number of unique values: \\n{col}, {data[col].nunique()}\\nValue Counts: \\n{data[col].value_counts()}\\n\\n----------------------\")"
      ],
      "metadata": {
        "id": "R__SOjgLmIZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Id6r0oYnFs"
      },
      "source": [
        "### Exploring Basic Models\n",
        "\n",
        "First we will try to generate the Business Name from the technical name, which should be a relatively straightforward problem. \n",
        "\n",
        "There are two basic approaches we can try. \n",
        "\n",
        "1. Similarity-based Model\n",
        "2. Rule-based Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caveats:\n",
        "- Where preprocessing already has generated the business label\n",
        "- unbalanced data set\n",
        "- small data set \n"
      ],
      "metadata": {
        "id": "XPFnxvWHrZIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess and normalize Text\n",
        "\n",
        "# in case text not english\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "# preprocessing steps\n",
        "  # remove excess white spaces\n",
        "  # remove underscores\n",
        "  # tokenize words\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for string in tqdm.tqdm(docs):\n",
        "    string = string.replace(\"_\", \" \")\n",
        "    string = string.translate(string.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    string = string.lower()\n",
        "    string = remove_accented_chars(string)\n",
        "    string = contractions.fix(string)\n",
        "    # lower case and remove special characters or whitespaces\n",
        "    string = re.sub(r'[^a-zA-Z0-9\\s]', '', string, flags=re.I|re.A)\n",
        "    string = string.strip()\n",
        "    norm_docs.append(string)\n",
        "\n",
        "  return norm_docs"
      ],
      "metadata": {
        "id": "P-eahSgLaZIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ff1c6579-f743-4b30-894f-cfdffbabac69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 7;\n",
              "                var nbb_unformatted_code = \"# preprocess and normalize Text\\n\\n# in case text not english\\ndef remove_accented_chars(text):\\n  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\\n  return text\\n\\n# preprocessing steps\\n  # remove excess white spaces\\n  # remove underscores\\n  # tokenize words\\ndef pre_process_corpus(docs):\\n  norm_docs = []\\n  for string in tqdm.tqdm(docs):\\n    string = string.replace(\\\"_\\\", \\\" \\\")\\n    string = string.translate(string.maketrans(\\\"\\\\n\\\\t\\\\r\\\", \\\"   \\\"))\\n    string = string.lower()\\n    string = remove_accented_chars(string)\\n    string = contractions.fix(string)\\n    # lower case and remove special characters or whitespaces\\n    string = re.sub(r'[^a-zA-Z0-9\\\\s]', '', string, flags=re.I|re.A)\\n    string = string.strip()\\n    norm_docs.append(string)\\n\\n  return norm_docs\";\n",
              "                var nbb_formatted_code = \"# preprocess and normalize Text\\n\\n# in case text not english\\ndef remove_accented_chars(text):\\n    text = (\\n        unicodedata.normalize(\\\"NFKD\\\", text)\\n        .encode(\\\"ascii\\\", \\\"ignore\\\")\\n        .decode(\\\"utf-8\\\", \\\"ignore\\\")\\n    )\\n    return text\\n\\n\\n# preprocessing steps\\n# remove excess white spaces\\n# remove underscores\\n# tokenize words\\ndef pre_process_corpus(docs):\\n    norm_docs = []\\n    for string in tqdm.tqdm(docs):\\n        string = string.replace(\\\"_\\\", \\\" \\\")\\n        string = string.translate(string.maketrans(\\\"\\\\n\\\\t\\\\r\\\", \\\"   \\\"))\\n        string = string.lower()\\n        string = remove_accented_chars(string)\\n        string = contractions.fix(string)\\n        # lower case and remove special characters or whitespaces\\n        string = re.sub(r\\\"[^a-zA-Z0-9\\\\s]\\\", \\\"\\\", string, flags=re.I | re.A)\\n        string = string.strip()\\n        norm_docs.append(string)\\n\\n    return norm_docs\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prep_tech = pre_process_corpus(data[\"Attribute_Technical_Name\"])\n"
      ],
      "metadata": {
        "id": "a1UVUjAZaZQd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90fa21c9-c82e-4810-d78a-598a1ef9fd19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4133/4133 [00:00<00:00, 85029.55it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 8;\n",
              "                var nbb_unformatted_code = \"prep_tech = pre_process_corpus(data[\\\"Attribute_Technical_Name\\\"])\";\n",
              "                var nbb_formatted_code = \"prep_tech = pre_process_corpus(data[\\\"Attribute_Technical_Name\\\"])\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prep_business = pre_process_corpus(data[\"Attribute_Business_Name\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OEIVuKgju_FF",
        "outputId": "efd8b1f1-ce8a-489e-9f9e-b863af78fb45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4133/4133 [00:00<00:00, 81617.46it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 9;\n",
              "                var nbb_unformatted_code = \"prep_business = pre_process_corpus(data[\\\"Attribute_Business_Name\\\"])\";\n",
              "                var nbb_formatted_code = \"prep_business = pre_process_corpus(data[\\\"Attribute_Business_Name\\\"])\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parse text into text pairs\n",
        "tech = pd.DataFrame(prep_tech, columns =[\"tech_name\"], dtype=\"string\")\n",
        "busi = pd.DataFrame(prep_business, columns =[\"busi_name\"], dtype=\"string\")\n",
        "attribute_df = pd.concat([tech,busi], axis=1)\n",
        "attribute_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cF9H7edVvLgB",
        "outputId": "a35a43ac-fe43-4ecf-fbfd-5474382df264"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2d355366-d6dc-4624-b3d5-b4656843b5c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tech_name</th>\n",
              "      <th>busi_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id</td>\n",
              "      <td>technical id of the patient</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gndr cd</td>\n",
              "      <td>gender code</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>livg arngmnt cd</td>\n",
              "      <td>living arrangement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mrtl stus cd</td>\n",
              "      <td>marital status code</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ocupatn cd</td>\n",
              "      <td>occupation code</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4128</th>\n",
              "      <td>config assetlist data source</td>\n",
              "      <td>config assetlist data source</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4129</th>\n",
              "      <td>attribute sample data</td>\n",
              "      <td>attribute sample data</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4130</th>\n",
              "      <td>property isdq</td>\n",
              "      <td>property isdq</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4131</th>\n",
              "      <td>property dq calculation</td>\n",
              "      <td>property dq calculation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4132</th>\n",
              "      <td>property dq output format</td>\n",
              "      <td>property dq output format</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4133 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d355366-d6dc-4624-b3d5-b4656843b5c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d355366-d6dc-4624-b3d5-b4656843b5c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d355366-d6dc-4624-b3d5-b4656843b5c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         tech_name                     busi_name\n",
              "0                               id   technical id of the patient\n",
              "1                          gndr cd                   gender code\n",
              "2                  livg arngmnt cd            living arrangement\n",
              "3                     mrtl stus cd           marital status code\n",
              "4                       ocupatn cd               occupation code\n",
              "...                            ...                           ...\n",
              "4128  config assetlist data source  config assetlist data source\n",
              "4129         attribute sample data         attribute sample data\n",
              "4130                 property isdq                 property isdq\n",
              "4131       property dq calculation       property dq calculation\n",
              "4132     property dq output format     property dq output format\n",
              "\n",
              "[4133 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 10;\n",
              "                var nbb_unformatted_code = \"# parse text into text pairs\\ntech = pd.DataFrame(prep_tech, columns =[\\\"tech_name\\\"], dtype=\\\"string\\\")\\nbusi = pd.DataFrame(prep_business, columns =[\\\"busi_name\\\"], dtype=\\\"string\\\")\\nattribute_df = pd.concat([tech,busi], axis=1)\\nattribute_df\";\n",
              "                var nbb_formatted_code = \"# parse text into text pairs\\ntech = pd.DataFrame(prep_tech, columns=[\\\"tech_name\\\"], dtype=\\\"string\\\")\\nbusi = pd.DataFrame(prep_business, columns=[\\\"busi_name\\\"], dtype=\\\"string\\\")\\nattribute_df = pd.concat([tech, busi], axis=1)\\nattribute_df\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle and define labels\n",
        "attribute_df = attribute_df.sample(frac=1, random_state=42)\n",
        "# X = attribute_df[\"tech_name\"]\n",
        "# Y = attribute_df[\"busi_name\"]\n",
        "\n",
        "# train test split\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, train_size=None, random_state=42)\n",
        "\n",
        "num_val_samples = int(0.15 * len(attribute_df))\n",
        "num_train_samples = len(attribute_df) - 2 * num_val_samples\n",
        "train_pairs = attribute_df[:num_train_samples]\n",
        "val_pairs = attribute_df[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = attribute_df[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(attribute_df)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "Ry8ijBWi7alm",
        "outputId": "f3afcd63-556a-4ff9-8577-b1ac7057dde6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4133 total pairs\n",
            "2895 training pairs\n",
            "619 validation pairs\n",
            "619 test pairs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 12;\n",
              "                var nbb_unformatted_code = \"# shuffle and define labels\\nattribute_df = attribute_df.sample(frac=1, random_state=42)\\n# X = attribute_df[\\\"tech_name\\\"]\\n# Y = attribute_df[\\\"busi_name\\\"]\\n\\n# train test split\\n# x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, train_size=None, random_state=42)\\n\\nnum_val_samples = int(0.15 * len(attribute_df))\\nnum_train_samples = len(attribute_df) - 2 * num_val_samples\\ntrain_pairs = attribute_df[:num_train_samples]\\nval_pairs = attribute_df[num_train_samples : num_train_samples + num_val_samples]\\ntest_pairs = attribute_df[num_train_samples + num_val_samples :]\\n\\nprint(f\\\"{len(attribute_df)} total pairs\\\")\\nprint(f\\\"{len(train_pairs)} training pairs\\\")\\nprint(f\\\"{len(val_pairs)} validation pairs\\\")\\nprint(f\\\"{len(test_pairs)} test pairs\\\")\";\n",
              "                var nbb_formatted_code = \"# shuffle and define labels\\nattribute_df = attribute_df.sample(frac=1, random_state=42)\\n# X = attribute_df[\\\"tech_name\\\"]\\n# Y = attribute_df[\\\"busi_name\\\"]\\n\\n# train test split\\n# x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, train_size=None, random_state=42)\\n\\nnum_val_samples = int(0.15 * len(attribute_df))\\nnum_train_samples = len(attribute_df) - 2 * num_val_samples\\ntrain_pairs = attribute_df[:num_train_samples]\\nval_pairs = attribute_df[num_train_samples : num_train_samples + num_val_samples]\\ntest_pairs = attribute_df[num_train_samples + num_val_samples :]\\n\\nprint(f\\\"{len(attribute_df)} total pairs\\\")\\nprint(f\\\"{len(train_pairs)} training pairs\\\")\\nprint(f\\\"{len(val_pairs)} validation pairs\\\")\\nprint(f\\\"{len(test_pairs)} test pairs\\\")\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configureations\n",
        "# batch_size\n",
        "# training epochs\n",
        "# latent diemsionality of the encoding space\n",
        "# number of samplesto train on\n",
        "# data path"
      ],
      "metadata": {
        "id": "q_cbtdNSvVvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "tech_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "busi_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        ")\n",
        "train_tech_texts = [pair[0] for pair in train_pairs]\n",
        "train_busi_texts = [pair[1] for pair in train_pairs]\n",
        "tech_vectorization.adapt(train_tech_texts)\n",
        "busi_vectorization.adapt(train_busi_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Wh_4eaKNx2Gf",
        "outputId": "1b2d56cb-9bec-436c-9f4d-1191d2537b68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 14;\n",
              "                var nbb_unformatted_code = \"# Vectorize the data\\n\\nvocab_size = 15000\\nsequence_length = 20\\nbatch_size = 64\\n\\ntech_vectorization = TextVectorization(\\n    max_tokens=vocab_size, output_mode=\\\"int\\\", output_sequence_length=sequence_length,\\n)\\nbusi_vectorization = TextVectorization(\\n    max_tokens=vocab_size,\\n    output_mode=\\\"int\\\",\\n    output_sequence_length=sequence_length + 1,\\n)\\ntrain_tech_texts = [pair[0] for pair in train_pairs]\\ntrain_busi_texts = [pair[1] for pair in train_pairs]\\ntech_vectorization.adapt(train_tech_texts)\\nbusi_vectorization.adapt(train_busi_texts)\";\n",
              "                var nbb_formatted_code = \"# Vectorize the data\\n\\nvocab_size = 15000\\nsequence_length = 20\\nbatch_size = 64\\n\\ntech_vectorization = TextVectorization(\\n    max_tokens=vocab_size,\\n    output_mode=\\\"int\\\",\\n    output_sequence_length=sequence_length,\\n)\\nbusi_vectorization = TextVectorization(\\n    max_tokens=vocab_size,\\n    output_mode=\\\"int\\\",\\n    output_sequence_length=sequence_length + 1,\\n)\\ntrain_tech_texts = [pair[0] for pair in train_pairs]\\ntrain_busi_texts = [pair[1] for pair in train_pairs]\\ntech_vectorization.adapt(train_tech_texts)\\nbusi_vectorization.adapt(train_busi_texts)\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = train_pairs.to_records(index=False)\n",
        "train_pairs = list(train_pairs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "8CLdzaZm3DHI",
        "outputId": "4732750b-58fc-45ba-9e5c-f0bd12af274a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-fc6dc5de8019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_records'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 23;\n",
              "                var nbb_unformatted_code = \"train_pairs = train_pairs.to_records(index=False)\\ntrain_pairs = list(train_pairs)\\ntrain_pairs\";\n",
              "                var nbb_formatted_code = \"train_pairs = train_pairs.to_records(index=False)\\ntrain_pairs = list(train_pairs)\\ntrain_pairs\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pairs = val_pairs.to_records(index=False)\n",
        "val_pairs = list(val_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Xce0p0Ad4YzS",
        "outputId": "683dea6a-ef5f-4aea-ccfc-80ae89894da0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 24;\n",
              "                var nbb_unformatted_code = \"val_pairs = val_pairs.to_records(index=False)\\nval_pairs = list(val_pairs)\";\n",
              "                var nbb_formatted_code = \"val_pairs = val_pairs.to_records(index=False)\\nval_pairs = list(val_pairs)\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pairs"
      ],
      "metadata": {
        "id": "OXMOfCZu5U4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(tech, busi):\n",
        "    tech = tech_vectorization(tech)\n",
        "    busi = busi_vectorization(busi)\n",
        "    return ({\"encoder_inputs\": tech, \"decoder_inputs\": busi[:, :-1],}, busi[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    tech_texts, busi_texts = zip(*pairs)\n",
        "    tech_texts = list(tech_texts)\n",
        "    busi_texts = list(busi_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((tech_texts, busi_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "e68eSTZ1Gd8m",
        "outputId": "44cb1fb1-0f94-4706-d68f-9a7acb4132f4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 30;\n",
              "                var nbb_unformatted_code = \"def format_dataset(tech, busi):\\n    tech = tech_vectorization(tech)\\n    busi = busi_vectorization(busi)\\n    return ({\\\"encoder_inputs\\\": tech, \\\"decoder_inputs\\\": busi[:, :-1],}, busi[:, 1:])\\n\\n\\ndef make_dataset(pairs):\\n    tech_texts, busi_texts = zip(*pairs)\\n    tech_texts = list(tech_texts)\\n    busi_texts = list(busi_texts)\\n    dataset = tf.data.Dataset.from_tensor_slices((tech_texts, busi_texts))\\n    dataset = dataset.batch(batch_size)\\n    dataset = dataset.map(format_dataset)\\n    return dataset.shuffle(2048).prefetch(16).cache()\\n\\n\\ntrain_ds = make_dataset(train_pairs)\\nval_ds = make_dataset(val_pairs)\";\n",
              "                var nbb_formatted_code = \"def format_dataset(tech, busi):\\n    tech = tech_vectorization(tech)\\n    busi = busi_vectorization(busi)\\n    return (\\n        {\\n            \\\"encoder_inputs\\\": tech,\\n            \\\"decoder_inputs\\\": busi[:, :-1],\\n        },\\n        busi[:, 1:],\\n    )\\n\\n\\ndef make_dataset(pairs):\\n    tech_texts, busi_texts = zip(*pairs)\\n    tech_texts = list(tech_texts)\\n    busi_texts = list(busi_texts)\\n    dataset = tf.data.Dataset.from_tensor_slices((tech_texts, busi_texts))\\n    dataset = dataset.batch(batch_size)\\n    dataset = dataset.map(format_dataset)\\n    return dataset.shuffle(2048).prefetch(16).cache()\\n\\n\\ntrain_ds = make_dataset(train_pairs)\\nval_ds = make_dataset(val_pairs)\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "E5j1nk2M7w39",
        "outputId": "873c4bc0-5dc6-44ff-c8a4-c5f446f2e7a7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 31;\n",
              "                var nbb_unformatted_code = \"for inputs, targets in train_ds.take(1):\\n    print(f'inputs[\\\"encoder_inputs\\\"].shape: {inputs[\\\"encoder_inputs\\\"].shape}')\\n    print(f'inputs[\\\"decoder_inputs\\\"].shape: {inputs[\\\"decoder_inputs\\\"].shape}')\\n    print(f\\\"targets.shape: {targets.shape}\\\")\";\n",
              "                var nbb_formatted_code = \"for inputs, targets in train_ds.take(1):\\n    print(f'inputs[\\\"encoder_inputs\\\"].shape: {inputs[\\\"encoder_inputs\\\"].shape}')\\n    print(f'inputs[\\\"decoder_inputs\\\"].shape: {inputs[\\\"decoder_inputs\\\"].shape}')\\n    print(f\\\"targets.shape: {targets.shape}\\\")\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AYzEcYubyGUY",
        "outputId": "dc5cca17-bb71-48ee-a51f-24037a87a280"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 34;\n",
              "                var nbb_unformatted_code = \"# Build the model\\nclass TransformerEncoder(layers.Layer):\\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\\n        super(TransformerEncoder, self).__init__(**kwargs)\\n        self.embed_dim = embed_dim\\n        self.dense_dim = dense_dim\\n        self.num_heads = num_heads\\n        self.attention = layers.MultiHeadAttention(\\n            num_heads=num_heads, key_dim=embed_dim\\n        )\\n        self.dense_proj = keras.Sequential(\\n            [layers.Dense(dense_dim, activation=\\\"relu\\\"), layers.Dense(embed_dim),]\\n        )\\n        self.layernorm_1 = layers.LayerNormalization()\\n        self.layernorm_2 = layers.LayerNormalization()\\n        self.supports_masking = True\\n\\n    def call(self, inputs, mask=None):\\n        if mask is not None:\\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\\\"int32\\\")\\n        attention_output = self.attention(\\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\\n        )\\n        proj_input = self.layernorm_1(inputs + attention_output)\\n        proj_output = self.dense_proj(proj_input)\\n        return self.layernorm_2(proj_input + proj_output)\\n\\n\\nclass PositionalEmbedding(layers.Layer):\\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\\n        super(PositionalEmbedding, self).__init__(**kwargs)\\n        self.token_embeddings = layers.Embedding(\\n            input_dim=vocab_size, output_dim=embed_dim\\n        )\\n        self.position_embeddings = layers.Embedding(\\n            input_dim=sequence_length, output_dim=embed_dim\\n        )\\n        self.sequence_length = sequence_length\\n        self.vocab_size = vocab_size\\n        self.embed_dim = embed_dim\\n\\n    def call(self, inputs):\\n        length = tf.shape(inputs)[-1]\\n        positions = tf.range(start=0, limit=length, delta=1)\\n        embedded_tokens = self.token_embeddings(inputs)\\n        embedded_positions = self.position_embeddings(positions)\\n        return embedded_tokens + embedded_positions\\n\\n    def compute_mask(self, inputs, mask=None):\\n        return tf.math.not_equal(inputs, 0)\\n\\n\\nclass TransformerDecoder(layers.Layer):\\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\\n        super(TransformerDecoder, self).__init__(**kwargs)\\n        self.embed_dim = embed_dim\\n        self.latent_dim = latent_dim\\n        self.num_heads = num_heads\\n        self.attention_1 = layers.MultiHeadAttention(\\n            num_heads=num_heads, key_dim=embed_dim\\n        )\\n        self.attention_2 = layers.MultiHeadAttention(\\n            num_heads=num_heads, key_dim=embed_dim\\n        )\\n        self.dense_proj = keras.Sequential(\\n            [layers.Dense(latent_dim, activation=\\\"relu\\\"), layers.Dense(embed_dim),]\\n        )\\n        self.layernorm_1 = layers.LayerNormalization()\\n        self.layernorm_2 = layers.LayerNormalization()\\n        self.layernorm_3 = layers.LayerNormalization()\\n        self.supports_masking = True\\n\\n    def call(self, inputs, encoder_outputs, mask=None):\\n        causal_mask = self.get_causal_attention_mask(inputs)\\n        if mask is not None:\\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\\\"int32\\\")\\n            padding_mask = tf.minimum(padding_mask, causal_mask)\\n\\n        attention_output_1 = self.attention_1(\\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\\n        )\\n        out_1 = self.layernorm_1(inputs + attention_output_1)\\n\\n        attention_output_2 = self.attention_2(\\n            query=out_1,\\n            value=encoder_outputs,\\n            key=encoder_outputs,\\n            attention_mask=padding_mask,\\n        )\\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\\n\\n        proj_output = self.dense_proj(out_2)\\n        return self.layernorm_3(out_2 + proj_output)\\n\\n    def get_causal_attention_mask(self, inputs):\\n        input_shape = tf.shape(inputs)\\n        batch_size, sequence_length = input_shape[0], input_shape[1]\\n        i = tf.range(sequence_length)[:, tf.newaxis]\\n        j = tf.range(sequence_length)\\n        mask = tf.cast(i >= j, dtype=\\\"int32\\\")\\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\\n        mult = tf.concat(\\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\\n            axis=0,\\n        )\\n        return tf.tile(mask, mult)\";\n",
              "                var nbb_formatted_code = \"# Build the model\\nclass TransformerEncoder(layers.Layer):\\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\\n        super(TransformerEncoder, self).__init__(**kwargs)\\n        self.embed_dim = embed_dim\\n        self.dense_dim = dense_dim\\n        self.num_heads = num_heads\\n        self.attention = layers.MultiHeadAttention(\\n            num_heads=num_heads, key_dim=embed_dim\\n        )\\n        self.dense_proj = keras.Sequential(\\n            [\\n                layers.Dense(dense_dim, activation=\\\"relu\\\"),\\n                layers.Dense(embed_dim),\\n            ]\\n        )\\n        self.layernorm_1 = layers.LayerNormalization()\\n        self.layernorm_2 = layers.LayerNormalization()\\n        self.supports_masking = True\\n\\n    def call(self, inputs, mask=None):\\n        if mask is not None:\\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\\\"int32\\\")\\n        attention_output = self.attention(\\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\\n        )\\n        proj_input = self.layernorm_1(inputs + attention_output)\\n        proj_output = self.dense_proj(proj_input)\\n        return self.layernorm_2(proj_input + proj_output)\\n\\n\\nclass PositionalEmbedding(layers.Layer):\\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\\n        super(PositionalEmbedding, self).__init__(**kwargs)\\n        self.token_embeddings = layers.Embedding(\\n            input_dim=vocab_size, output_dim=embed_dim\\n        )\\n        self.position_embeddings = layers.Embedding(\\n            input_dim=sequence_length, output_dim=embed_dim\\n        )\\n        self.sequence_length = sequence_length\\n        self.vocab_size = vocab_size\\n        self.embed_dim = embed_dim\\n\\n    def call(self, inputs):\\n        length = tf.shape(inputs)[-1]\\n        positions = tf.range(start=0, limit=length, delta=1)\\n        embedded_tokens = self.token_embeddings(inputs)\\n        embedded_positions = self.position_embeddings(positions)\\n        return embedded_tokens + embedded_positions\\n\\n    def compute_mask(self, inputs, mask=None):\\n        return tf.math.not_equal(inputs, 0)\\n\\n\\nclass TransformerDecoder(layers.Layer):\\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\\n        super(TransformerDecoder, self).__init__(**kwargs)\\n        self.embed_dim = embed_dim\\n        self.latent_dim = latent_dim\\n        self.num_heads = num_heads\\n        self.attention_1 = layers.MultiHeadAttention(\\n            num_heads=num_heads, key_dim=embed_dim\\n        )\\n        self.attention_2 = layers.MultiHeadAttention(\\n            num_heads=num_heads, key_dim=embed_dim\\n        )\\n        self.dense_proj = keras.Sequential(\\n            [\\n                layers.Dense(latent_dim, activation=\\\"relu\\\"),\\n                layers.Dense(embed_dim),\\n            ]\\n        )\\n        self.layernorm_1 = layers.LayerNormalization()\\n        self.layernorm_2 = layers.LayerNormalization()\\n        self.layernorm_3 = layers.LayerNormalization()\\n        self.supports_masking = True\\n\\n    def call(self, inputs, encoder_outputs, mask=None):\\n        causal_mask = self.get_causal_attention_mask(inputs)\\n        if mask is not None:\\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\\\"int32\\\")\\n            padding_mask = tf.minimum(padding_mask, causal_mask)\\n\\n        attention_output_1 = self.attention_1(\\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\\n        )\\n        out_1 = self.layernorm_1(inputs + attention_output_1)\\n\\n        attention_output_2 = self.attention_2(\\n            query=out_1,\\n            value=encoder_outputs,\\n            key=encoder_outputs,\\n            attention_mask=padding_mask,\\n        )\\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\\n\\n        proj_output = self.dense_proj(out_2)\\n        return self.layernorm_3(out_2 + proj_output)\\n\\n    def get_causal_attention_mask(self, inputs):\\n        input_shape = tf.shape(inputs)\\n        batch_size, sequence_length = input_shape[0], input_shape[1]\\n        i = tf.range(sequence_length)[:, tf.newaxis]\\n        j = tf.range(sequence_length)\\n        mask = tf.cast(i >= j, dtype=\\\"int32\\\")\\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\\n        mult = tf.concat(\\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\\n            axis=0,\\n        )\\n        return tf.tile(mask, mult)\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WsKXYwLZ8HLl",
        "outputId": "3b71ab31-4a3c-4fbb-dc89-8b9d87ea3b29"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 37;\n",
              "                var nbb_unformatted_code = \"embed_dim = 256\\nlatent_dim = 2048\\nnum_heads = 8\\n\\nencoder_inputs = keras.Input(shape=(None,), dtype=\\\"int64\\\", name=\\\"encoder_inputs\\\")\\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\\nencoder = keras.Model(encoder_inputs, encoder_outputs)\\n\\ndecoder_inputs = keras.Input(shape=(None,), dtype=\\\"int64\\\", name=\\\"decoder_inputs\\\")\\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\\\"decoder_state_inputs\\\")\\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\\nx = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\\nx = layers.Dropout(0.5)(x)\\ndecoder_outputs = layers.Dense(vocab_size, activation=\\\"softmax\\\")(x)\\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\\n\\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\\ntransformer = keras.Model(\\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\\\"transformer\\\"\\n)\";\n",
              "                var nbb_formatted_code = \"embed_dim = 256\\nlatent_dim = 2048\\nnum_heads = 8\\n\\nencoder_inputs = keras.Input(shape=(None,), dtype=\\\"int64\\\", name=\\\"encoder_inputs\\\")\\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\\nencoder = keras.Model(encoder_inputs, encoder_outputs)\\n\\ndecoder_inputs = keras.Input(shape=(None,), dtype=\\\"int64\\\", name=\\\"decoder_inputs\\\")\\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\\\"decoder_state_inputs\\\")\\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\\nx = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\\nx = layers.Dropout(0.5)(x)\\ndecoder_outputs = layers.Dense(vocab_size, activation=\\\"softmax\\\")(x)\\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\\n\\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\\ntransformer = keras.Model(\\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\\\"transformer\\\"\\n)\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "epochs = 30  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "# train the model with fit\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
        "# save model\n",
        "transformer.save(\"transformer_RNN_attribute_labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7QR8b52jyI2y",
        "outputId": "0e00ebd4-002e-4fac-a5f1-6cc51f6c055e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "46/46 [==============================] - 7s 81ms/step - loss: 0.0940 - accuracy: 0.7945 - val_loss: 0.0940 - val_accuracy: 0.7950\n",
            "Epoch 2/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0926 - accuracy: 0.8008 - val_loss: 0.0944 - val_accuracy: 0.7929\n",
            "Epoch 3/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0925 - accuracy: 0.8008 - val_loss: 0.0938 - val_accuracy: 0.7950\n",
            "Epoch 4/30\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.0920 - accuracy: 0.8029 - val_loss: 0.0940 - val_accuracy: 0.7950\n",
            "Epoch 5/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0920 - accuracy: 0.8029 - val_loss: 0.0940 - val_accuracy: 0.7950\n",
            "Epoch 6/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0920 - accuracy: 0.8004 - val_loss: 0.0937 - val_accuracy: 0.7955\n",
            "Epoch 7/30\n",
            "46/46 [==============================] - 3s 71ms/step - loss: 0.0918 - accuracy: 0.8016 - val_loss: 0.0934 - val_accuracy: 0.7950\n",
            "Epoch 8/30\n",
            "46/46 [==============================] - 3s 75ms/step - loss: 0.0922 - accuracy: 0.8024 - val_loss: 0.0929 - val_accuracy: 0.7959\n",
            "Epoch 9/30\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.0919 - accuracy: 0.8019 - val_loss: 0.0938 - val_accuracy: 0.7934\n",
            "Epoch 10/30\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.0916 - accuracy: 0.8036 - val_loss: 0.0934 - val_accuracy: 0.7955\n",
            "Epoch 11/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0917 - accuracy: 0.8034 - val_loss: 0.0934 - val_accuracy: 0.7955\n",
            "Epoch 12/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0917 - accuracy: 0.8033 - val_loss: 0.0940 - val_accuracy: 0.8001\n",
            "Epoch 13/30\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.0917 - accuracy: 0.8025 - val_loss: 0.0935 - val_accuracy: 0.7946\n",
            "Epoch 14/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0917 - accuracy: 0.8022 - val_loss: 0.0937 - val_accuracy: 0.7946\n",
            "Epoch 15/30\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.0918 - accuracy: 0.8024 - val_loss: 0.0935 - val_accuracy: 0.7950\n",
            "Epoch 16/30\n",
            "46/46 [==============================] - 3s 67ms/step - loss: 0.0918 - accuracy: 0.8031 - val_loss: 0.0929 - val_accuracy: 0.7955\n",
            "Epoch 17/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0914 - accuracy: 0.8008 - val_loss: 0.0940 - val_accuracy: 0.7946\n",
            "Epoch 18/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0917 - accuracy: 0.8008 - val_loss: 0.0936 - val_accuracy: 0.7950\n",
            "Epoch 19/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0918 - accuracy: 0.8027 - val_loss: 0.0932 - val_accuracy: 0.7946\n",
            "Epoch 20/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0916 - accuracy: 0.8032 - val_loss: 0.0933 - val_accuracy: 0.7946\n",
            "Epoch 21/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0917 - accuracy: 0.8023 - val_loss: 0.0930 - val_accuracy: 0.7950\n",
            "Epoch 22/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0915 - accuracy: 0.8041 - val_loss: 0.0932 - val_accuracy: 0.7946\n",
            "Epoch 23/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0913 - accuracy: 0.8030 - val_loss: 0.0932 - val_accuracy: 0.7984\n",
            "Epoch 24/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0915 - accuracy: 0.8020 - val_loss: 0.0931 - val_accuracy: 0.7950\n",
            "Epoch 25/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0911 - accuracy: 0.8044 - val_loss: 0.0932 - val_accuracy: 0.7929\n",
            "Epoch 26/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0914 - accuracy: 0.8035 - val_loss: 0.0930 - val_accuracy: 0.7950\n",
            "Epoch 27/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0913 - accuracy: 0.8026 - val_loss: 0.0926 - val_accuracy: 0.7946\n",
            "Epoch 28/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0914 - accuracy: 0.8033 - val_loss: 0.0926 - val_accuracy: 0.7946\n",
            "Epoch 29/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0912 - accuracy: 0.8046 - val_loss: 0.0930 - val_accuracy: 0.7950\n",
            "Epoch 30/30\n",
            "46/46 [==============================] - 3s 68ms/step - loss: 0.0912 - accuracy: 0.8038 - val_loss: 0.0931 - val_accuracy: 0.7950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 150). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: transformer_RNN_attribute_labels/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: transformer_RNN_attribute_labels/assets\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 40;\n",
              "                var nbb_unformatted_code = \"# Compile the model\\nepochs = 30  # This should be at least 30 for convergence\\n\\ntransformer.summary()\\ntransformer.compile(\\n    \\\"rmsprop\\\", loss=\\\"sparse_categorical_crossentropy\\\", metrics=[\\\"accuracy\\\"]\\n)\\n# train the model with fit\\ntransformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\\n# save model\\ntransformer.save(\\\"transformer_RNN_attribute_labels\\\")\";\n",
              "                var nbb_formatted_code = \"# Compile the model\\nepochs = 30  # This should be at least 30 for convergence\\n\\ntransformer.summary()\\ntransformer.compile(\\n    \\\"rmsprop\\\", loss=\\\"sparse_categorical_crossentropy\\\", metrics=[\\\"accuracy\\\"]\\n)\\n# train the model with fit\\ntransformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\\n# save model\\ntransformer.save(\\\"transformer_RNN_attribute_labels\\\")\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with fit\n",
        "# save model"
      ],
      "metadata": {
        "id": "ORbuz331yM-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "do2AkeX2YnFs"
      },
      "source": [
        "<div style=\"background:#EEEDF5;color:#303030\"><div style=\"margin-left: .2cm\">\n",
        "    <b>Comments:</b>\n",
        "</div></div>\n",
        "\n",
        "<div style=\"margin-left: 0.4cm\">\n",
        "    This Markdown cell is a template to write extended comments. e.g. explain the approach taken, or concepts behind algorithm chosen. Aim at having 2 lines at least. Anything smaller can be directly integrated in the code, or gathered together with other comments to form a bigger paragraph. Since this block is within a div element, you need to use html formatting syntax, not markdown\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "WydTl7kyYnFs"
      },
      "source": [
        "<div style=\"background:#EEEDF5;border:0.1cm solid #FDB100;color:#303030\">\n",
        "    <div style=\"margin: 0.2cm 0.2cm 0.2cm 0.2cm\">\n",
        "        <b style=\"color:#FDB100\">Title:</b> \n",
        "        This is a an example yellow box\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "j4hyUQSwYnFt"
      },
      "source": [
        "<div style=\"background:#EEEDF5;border:0.1cm solid #00BAE5;color:#303030\">\n",
        "    <div style=\"margin: 0.2cm 0.2cm 0.2cm 0.2cm\">\n",
        "        <b style=\"color:#00BAE5\">Note:</b>\n",
        "        This is an example Blue Box\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RGx4DtmYnFt"
      },
      "source": [
        "<div style=\"background:#EEEDF5;border:0.1cm solid #EF475B;color:#303030\">\n",
        "    <div style=\"margin: 0.2cm 0.2cm 0.2cm 0.2cm\">\n",
        "        <b style=\"color:#EF475B\">Warning:</b> \n",
        "        This is an example red box\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Jw7PIJHdYnFt"
      },
      "source": [
        "<a id='P2' name=\"P2\"></a>\n",
        "## [Part 2 title here](#P0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiyT-faAYnFt"
      },
      "source": [
        "<a id='P3' name=\"P3\"></a>\n",
        "## [Part 3 title here](#P0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "CA0lXBcZYnFu"
      },
      "source": [
        "<a id='CL'></a>\n",
        "## [Conclusion](#P0)\n",
        "\n",
        "This is a pre-written conclusion in which we have nice figures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC2g6fGxYnFu"
      },
      "source": [
        "<div style=\"border-top:0.1cm solid #EF475B\"></div>\n",
        "    <strong><a href='#Q0'><div style=\"text-align: right\"> <h3>End of this Notebook.</h3></div></a></strong>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Sidecar_Project_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
